{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef30bb2d-08d4-4f24-a1b6-0cdbaf373f2a",
   "metadata": {},
   "source": [
    "### Useful links:\n",
    "* https://medium.com/analytics-vidhya/music-lyrics-analysis-using-natural-language-processing-7647922241c0\n",
    "* https://machinelearningmastery.com/how-to-develop-a-word-level-neural-language-model-in-keras/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f0bad6-3ad2-4726-b070-ad1ff2b4b19c",
   "metadata": {},
   "source": [
    "## Load, clean, tokenize, and create sequences of lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "562241ef-aee3-40c6-9c1a-9dd93be7524b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heartbreak drowned sorrows in a large steak\n",
      "\n",
      "Why you always all on my back?\n",
      "Why you gotta do me like that?\n",
      "Why you gotta act like a bitch when I'm with you?\n",
      "Baby girl, I'm blue\n",
      "\n",
      "Because you treat me like shit\n",
      "I paid for the bed and never even slept in it\n",
      "I paid for that crib I never stepped foot in\n",
      "And now somebody else is eating all the pudding\n",
      "Things change, now my dashboard wooden\n",
      "All black Benz, like a young Doc Gooden\n",
      "Dark shades, 'cause I'm stone crazy\n",
      "Girl, we grown, stop playin' on my ph\n"
     ]
    }
   ],
   "source": [
    "# load lyrics into memory\n",
    "def load_lyrics(filename):\n",
    "    # open the lyrics as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "# load lyrics\n",
    "in_filename = 'texts/Action Bronson_lyrics_of_50_songs_v2.txt'\n",
    "lyrics = load_lyrics(in_filename)\n",
    "print(lyrics[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "753d11c1-aca2-4e0d-90bf-b2159a36b5e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['heartbreak', 'drowned', 'sorrows', 'in', 'a', 'large', 'steak', 'why', 'you', 'always', 'all', 'on', 'my', 'back', 'why', 'you', 'gotta', 'do', 'me', 'like', 'that', 'why', 'you', 'gotta', 'act', 'like', 'a', 'bitch', 'when', 'im', 'with', 'you', 'baby', 'girl', 'im', 'blue', 'because', 'you', 'treat', 'me', 'like', 'shit', 'i', 'paid', 'for', 'the', 'bed', 'and', 'never', 'even', 'slept', 'in', 'it', 'i', 'paid', 'for', 'that', 'crib', 'i', 'never', 'stepped', 'foot', 'in', 'and', 'now', 'somebody', 'else', 'is', 'eating', 'all', 'the', 'pudding', 'things', 'change', 'now', 'my', 'dashboard', 'wooden', 'all', 'black', 'benz', 'like', 'a', 'young', 'doc', 'gooden', 'dark', 'shades', 'cause', 'im', 'stone', 'crazy', 'girl', 'we', 'grown', 'stop', 'playin', 'on', 'my', 'phone', 'baby', 'all', 'your', 'childish', 'attempts', 'to', 'make', 'me', 'angry', 'fall', 'short', 'which', 'only', 'fuels', 'the', 'rage', 'you', 'have', 'because', 'you', 'have', 'nothing', 'understandable', 'im', 'shinin', 'brilliant', 'with', 'five', 'brazilians', 'there', 'were', 'times', 'i', 'used', 'to', 'hide', 'my', 'feelings', 'now', 'im', 'butt', 'naked', 'in', 'the', 'lamborghini', 'and', 'motherfuckers', 'cant', 'see', 'me', 'wait', 'til', 'this', 'chick', 'see', 'me', 'on', 'tv', 'i', 'make', 'the', 'shit', 'look', 'easy', 'who', 'wouldve', 'thought', 'i', 'hit', 'you', 'right', 'back', 'why', 'you', 'always', 'all', 'on', 'my', 'back', 'why', 'you', 'gotta', 'do', 'me', 'like', 'that', 'why', 'you', 'gotta', 'act', 'like', 'a', 'bitch', 'when', 'im', 'with', 'you', 'baby', 'girl', 'im']\n",
      "Total Tokens: 21030\n",
      "Unique Tokens: 4316\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "    # replace '--' with a space ' '\n",
    "    doc = doc.replace('--', ' ')\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # remove punctuation from each token\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # make lower case\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    return tokens\n",
    "\n",
    "# clean document\n",
    "tokens = clean_doc(lyrics)\n",
    "print(tokens[:200])\n",
    "print('Total Tokens: %d' % len(tokens))\n",
    "print('Unique Tokens: %d' % len(set(tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f757711-acf4-4482-876c-226864df3139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences: 20979\n"
     ]
    }
   ],
   "source": [
    "# organize into sequences of tokens\n",
    "length = 50 + 1\n",
    "sequences = list()\n",
    "for i in range(length, len(tokens)):\n",
    "    # select sequence of tokens\n",
    "    seq = tokens[i-length:i]\n",
    "    # convert into a line\n",
    "    line = ' '.join(seq)\n",
    "    # store\n",
    "    sequences.append(line)\n",
    "print('Total Sequences: %d' % len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "baf24d43-b7f2-44c2-b177-d31151a9bfb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save tokens to file, one dialog per line\n",
    "def save_doc(lines, filename):\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename, 'w')\n",
    "    file.write(data)\n",
    "    file.close()\n",
    "    \n",
    "# save sequences to file\n",
    "out_filename = 'texts/action_bronson_sequences.txt'\n",
    "save_doc(sequences, out_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d333ab90-a495-4bfa-a279-6ceeed714aba",
   "metadata": {},
   "source": [
    "## Train Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8dbe2c1-78ba-4c15-b884-1803ee13ef9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load\n",
    "in_filename = 'texts/action_bronson_sequences.txt'\n",
    "lyrics = load_lyrics(in_filename)\n",
    "lines = lyrics.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "359ddbd2-b0c7-44f2-bfb0-a5e0b0a56dc5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m array\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dump\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from numpy import array\n",
    "from pickle import dump\n",
    "from tf.preprocessing.text import Tokenizer\n",
    "from tf.utils import to_categorical\n",
    "from tf.models import Sequential\n",
    "from tf.layers import Dense\n",
    "from tf.layers import LSTM\n",
    "from tf.layers import Embedding\n",
    "\n",
    "# load\n",
    "in_filename = 'texts/action_bronson_sequences.txt'\n",
    "lyrics = load_lyrics(in_filename)\n",
    "lines = lyrics.split('\\n')\n",
    "\n",
    "# integer encode sequences of words\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(lines)\n",
    "sequences = tokenizer.texts_to_sequences(lines)\n",
    "# vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# separate into input and output\n",
    "sequences = array(sequences)\n",
    "X, y = sequences[:,:-1], sequences[:,-1]\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "seq_length = X.shape[1]\n",
    "\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 50, input_length=seq_length))\n",
    "model.add(LSTM(100, return_sequences=True))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "print(model.summary())\n",
    "# compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit model\n",
    "model.fit(X, y, batch_size=128, epochs=100)\n",
    "\n",
    "# save the model to file\n",
    "model.save('model.h5')\n",
    "# save the tokenizer\n",
    "dump(tokenizer, open('tokenizer.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a593bc96-0ec3-4e72-ba3e-5613d29d26b2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m array\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dump\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tokenizer\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m to_categorical\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from pickle import dump\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, 'r')\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text\n",
    "\n",
    "# load\n",
    "in_filename = 'republic_sequences.txt'\n",
    "doc = load_doc(in_filename)\n",
    "lines = doc.split('\\n')\n",
    "\n",
    "# integer encode sequences of words\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(lines)\n",
    "sequences = tokenizer.texts_to_sequences(lines)\n",
    "# vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# separate into input and output\n",
    "sequences = array(sequences)\n",
    "X, y = sequences[:,:-1], sequences[:,-1]\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "seq_length = X.shape[1]\n",
    "\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 50, input_length=seq_length))\n",
    "model.add(LSTM(100, return_sequences=True))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "print(model.summary())\n",
    "# compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit model\n",
    "model.fit(X, y, batch_size=128, epochs=100)\n",
    "\n",
    "# save the model to file\n",
    "model.save('model.h5')\n",
    "# save the tokenizer\n",
    "dump(tokenizer, open('tokenizer.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad3268de-859a-4738-be5a-0908f04bbeeb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
